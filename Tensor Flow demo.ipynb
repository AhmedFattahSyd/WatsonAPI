{
    "nbformat_minor": 1, 
    "metadata": {
        "language_info": {
            "nbconvert_exporter": "python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "name": "python", 
            "file_extension": ".py", 
            "version": "2.7.11", 
            "mimetype": "text/x-python"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "source": "# Tensor Flow demo\nThis notebook demonsrate the use of Tensor Flow for Deep Learning", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 1, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Install prerequiste libraries\n#!pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 2, 
            "metadata": {}, 
            "source": "# import tensorflow python library into notebook\nimport tensorflow as tf\n# verify tensorflow library is loaded and is functioning properly\n# Create TensorFlow object called hello_constant\nhello_constant = tf.constant('Hello World!')\n\nwith tf.Session() as sess:\n    # Run the tf.constant operation in the session\n    output = sess.run(hello_constant)\n    print(output)", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Hello World!\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "## simple example with MNIST dataset\nThe following section performs simple learning on the MNIST dataset", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 3, 
            "metadata": {}, 
            "source": "# get MNIST dataset\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 4, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# create a placeholder to store digits\n# a 2-D tensor of floating-point numbers, with a shape [None, 784]\n# 784 = 28x28 which are the dimension of the digits\n# this flaten the structure into just one dimension (for simplicity)\nx = tf.placeholder(tf.float32, [None, 784])\n# we now create variables to hold the weights\n# the dimension of W (weights) is 784 (inputs) X 10 (output: 0-9)\n# b is the constant input (10: 0-9)\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n# we now implement our model using softmax\n# y is the output of the model\n# nn stands for neural network, a set of avalable functions including softmax\ny = tf.nn.softmax(tf.matmul(x, W) + b)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "## Training the model\nThe following cell trains the model", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 6, 
            "metadata": {}, 
            "source": "# we now train the model and measure the residul error\n# The residual error, the 'loss' function or cross-entropy is the difference between the correct lable and the model output\n# let's define a place folder for the correct output\ny_ = tf.placeholder(tf.float32, [None, 10])\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n# we now define the training step which use GradientDescend algorithm\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n# finally the initialisation setp\n# the folowing statement goves warning about tf.initialize_all_variables() being depratcted\n# we replace it with tf.global_variables_initializer()\n# init = tf.initialize_all_variables()\ninit = tf.global_variables_initializer()\n# let's now initialise the model\nsess = tf.Session()\nsess.run(init)\n# and run the training step 1000 times minimising the error\nfor i in range(1000):\n  batch_xs, batch_ys = mnist.train.next_batch(100)\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "## Evaluating the model", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 7, 
            "metadata": {}, 
            "source": "# we now evaluate the model how is it performing\n# tf.argmax(y,1) is the label our model thinks is most likely for each input\n# tf.argmax(y_,1) is the correct label\n# if the above two terms are equal, the model predication is correct\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n# the above gives us a list of booleans\n# To determine what fraction are correct, we cast to floating point numbers and then take the mean. \n# For example, [True, False, True, True] would become [1,0,1,1] which would become 0.75\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "0.9175\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "outputs": [], 
            "cell_type": "code"
        }
    ]
}