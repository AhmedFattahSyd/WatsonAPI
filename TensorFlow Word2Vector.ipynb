{
    "nbformat_minor": 1, 
    "metadata": {
        "language_info": {
            "nbconvert_exporter": "python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "name": "python", 
            "file_extension": ".py", 
            "version": "2.7.11", 
            "mimetype": "text/x-python"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python2-spark21", 
            "display_name": "Python 2 with Spark 2.1"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "source": "# TensorFlow Word2Vector", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "This notebook implemnet a simple word2vector algortithm ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 1, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# set directive for import (see note above)\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 2, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# import prerequiste libraries\nimport collections\nimport math\nimport os\nimport random\nimport zipfile\n\nimport numpy as np\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 3, 
            "metadata": {}, 
            "source": "# Step 1: Download the data.\nurl = 'http://mattmahoney.net/dc/'\ndef maybe_download(filename, expected_bytes):\n  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n  if not os.path.exists(filename):\n    filename, _ = urllib.request.urlretrieve(url + filename, filename)\n  statinfo = os.stat(filename)\n  if statinfo.st_size == expected_bytes:\n    print('Found and verified', filename)\n  else:\n    print(statinfo.st_size)\n    raise Exception(\n        'Failed to verify ' + filename + '. Can you get to it with a browser?')\n  return filename\n\nfilename = maybe_download('text8.zip', 31344016)", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Found and verified text8.zip\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 4, 
            "metadata": {}, 
            "source": "# Read the data into a list of strings.\ndef read_data(filename):\n  \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n  with zipfile.ZipFile(filename) as f:\n    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n  return data\n\nvocabulary = read_data(filename)\nprint('Data size', len(vocabulary))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Data size 17005207\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 5, 
            "metadata": {}, 
            "source": "# Step 2: Build the dictionary and replace rare words with UNK token.\nvocabulary_size = 50000\n\n\ndef build_dataset(words, n_words):\n  \"\"\"Process raw inputs into a dataset.\"\"\"\n  count = [['UNK', -1]]\n  count.extend(collections.Counter(words).most_common(n_words - 1))\n  dictionary = dict()\n  for word, _ in count:\n    dictionary[word] = len(dictionary)\n  data = list()\n  unk_count = 0\n  for word in words:\n    if word in dictionary:\n      index = dictionary[word]\n    else:\n      index = 0  # dictionary['UNK']\n      unk_count += 1\n    data.append(index)\n  count[0][1] = unk_count\n  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n  return data, count, dictionary, reversed_dictionary\n\ndata, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n                                                            vocabulary_size)\ndel vocabulary  # Hint to reduce memory.\nprint('Most common words (+UNK)', count[:5])\nprint('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n\ndata_index = 0", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\nSample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 6, 
            "metadata": {}, 
            "source": "# Step 3: Function to generate a training batch for the skip-gram model.\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0\n  assert num_skips <= 2 * skip_window\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)\n  if data_index + span > len(data):\n    data_index = 0\n  buffer.extend(data[data_index:data_index + span])\n  data_index += span\n  for i in range(batch_size // num_skips):\n    target = skip_window  # target label at the center of the buffer\n    targets_to_avoid = [skip_window]\n    for j in range(num_skips):\n      while target in targets_to_avoid:\n        target = random.randint(0, span - 1)\n      targets_to_avoid.append(target)\n      batch[i * num_skips + j] = buffer[skip_window]\n      labels[i * num_skips + j, 0] = buffer[target]\n    if data_index == len(data):\n      buffer[:] = data[:span]\n      data_index = span\n    else:\n      buffer.append(data[data_index])\n      data_index += 1\n  # Backtrack a little bit to avoid skipping words in the end of a batch\n  data_index = (data_index + len(data) - span) % len(data)\n  return batch, labels\n\nbatch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\nfor i in range(8):\n  print(batch[i], reverse_dictionary[batch[i]],\n        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "3084 originated -> 12 as\n3084 originated -> 5239 anarchism\n12 as -> 3084 originated\n12 as -> 6 a\n6 a -> 12 as\n6 a -> 195 term\n195 term -> 6 a\n195 term -> 2 of\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 7, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Step 4: Build and train a skip-gram model.\n\nbatch_size = 128\nembedding_size = 128  # Dimension of the embedding vector.\nskip_window = 1       # How many words to consider left and right.\nnum_skips = 2         # How many times to reuse an input to generate a label.\n\n# We pick a random validation set to sample nearest neighbors. Here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent.\nvalid_size = 16     # Random set of words to evaluate similarity on.\nvalid_window = 100  # Only pick dev samples in the head of the distribution.\nvalid_examples = np.random.choice(valid_window, valid_size, replace=False)\nnum_sampled = 64    # Number of negative examples to sample.\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n  # Ops and variables pinned to the CPU because of missing GPU implementation\n  with tf.device('/cpu:0'):\n    # Look up embeddings for inputs.\n    embeddings = tf.Variable(\n        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n    # Construct the variables for the NCE loss\n    nce_weights = tf.Variable(\n        tf.truncated_normal([vocabulary_size, embedding_size],\n                            stddev=1.0 / math.sqrt(embedding_size)))\n    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n  # Compute the average NCE loss for the batch.\n  # tf.nce_loss automatically draws a new sample of the negative labels each\n  # time we evaluate the loss.\n  loss = tf.reduce_mean(\n      tf.nn.nce_loss(weights=nce_weights,\n                     biases=nce_biases,\n                     labels=train_labels,\n                     inputs=embed,\n                     num_sampled=num_sampled,\n                     num_classes=vocabulary_size))\n\n  # Construct the SGD optimizer using a learning rate of 1.0.\n  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\n  # Compute the cosine similarity between minibatch examples and all embeddings.\n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n  normalized_embeddings = embeddings / norm\n  valid_embeddings = tf.nn.embedding_lookup(\n      normalized_embeddings, valid_dataset)\n  similarity = tf.matmul(\n      valid_embeddings, normalized_embeddings, transpose_b=True)\n\n  # Add variable initializer.\n  init = tf.global_variables_initializer()", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 8, 
            "metadata": {}, 
            "source": "# Step 5: Begin training.\nnum_steps = 100001\n\nwith tf.Session(graph=graph) as session:\n  # We must initialize all variables before we use them.\n  init.run()\n  print('Initialized')\n\n  average_loss = 0\n  for step in xrange(num_steps):\n    batch_inputs, batch_labels = generate_batch(\n        batch_size, num_skips, skip_window)\n    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n\n    # We perform one update step by evaluating the optimizer op (including it\n    # in the list of returned values for session.run()\n    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n    average_loss += loss_val\n\n    if step % 2000 == 0:\n      if step > 0:\n        average_loss /= 2000\n      # The average loss is an estimate of the loss over the last 2000 batches.\n      print('Average loss at step ', step, ': ', average_loss)\n      average_loss = 0\n\n    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n    if step % 10000 == 0:\n      sim = similarity.eval()\n      for i in xrange(valid_size):\n        valid_word = reverse_dictionary[valid_examples[i]]\n        top_k = 8  # number of nearest neighbors\n        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n        log_str = 'Nearest to %s:' % valid_word\n        for k in xrange(top_k):\n          close_word = reverse_dictionary[nearest[k]]\n          log_str = '%s %s,' % (log_str, close_word)\n        print(log_str)\n  final_embeddings = normalized_embeddings.eval()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Initialized\nAverage loss at step  0 :  293.423065186\nNearest to can: dandies, schoenberg, art, rectify, moulds, writs, hadadezer, henchman,\nNearest to while: bart, sends, promotes, fptp, appeasement, emulators, brake, unarmed,\nNearest to however: complementary, yue, bronchitis, zhao, antagonist, philippi, excepting, girardeau,\nNearest to some: anatomy, blizzard, sava, newsgroup, ntfs, terrains, wavefunction, infoplease,\nNearest to was: register, dogged, bianchi, resurface, penalized, pr, smiling, valerius,\nNearest to united: quantify, dianne, jean, wallpaper, abortion, blogging, composes, martyn,\nNearest to for: korzybski, ignition, monkeys, insurgency, hominidae, anguish, cemetery, md,\nNearest to will: seaborg, paraguay, epiphanes, preventing, ahijah, oldest, mortem, shabazz,\nNearest to eight: zeppelin, dualist, multicast, kent, cherished, room, decay, rasterization,\nNearest to on: elmore, pancras, constitutionally, muni, oscillating, silicon, headaches, streamlining,\nNearest to or: houghton, obedient, hallucinogens, anconia, ecotourism, hesitant, concerned, intercalary,\nNearest to new: nour, hurd, magnolia, aloysius, erd, goitre, son, opulent,\nNearest to state: dantzig, placeholder, dundee, innodb, carnage, comprised, intermixed, retelling,\nNearest to an: infallible, foothold, contained, girdle, hotly, sholom, midway, alvarez,\nNearest to three: xia, shona, arnstadt, basie, collaboration, tetrachloride, carefree, disagrees,\nNearest to of: jonathan, flunitrazepam, shortage, amalthea, prevailing, adjectival, faber, diarrhea,\nAverage loss at step  2000 :  113.605286249\nAverage loss at step  4000 :  52.927898824\nAverage loss at step  6000 :  33.3767920406\nAverage loss at step  8000 :  23.3630198243\nAverage loss at step  10000 :  17.9588385437\nNearest to can: art, ep, ominous, observing, readily, calf, screenplay, qur,\nNearest to while: brake, promotes, appeasement, sends, bart, unarmed, crew, coldest,\nNearest to however: complementary, arguments, well, vocals, updated, yields, excepting, UNK,\nNearest to some: anatomy, newsgroup, bckgr, vocals, mothers, comic, UNK, resources,\nNearest to was: is, were, has, altenberg, vocals, rn, midas, damned,\nNearest to united: abortion, wallpaper, field, carries, jean, geology, whipped, martha,\nNearest to for: of, and, in, on, vocals, reginae, to, dunes,\nNearest to will: oldest, here, seaborg, coke, town, paraguay, preventing, clear,\nNearest to eight: nine, zero, agave, seven, six, vs, reginae, cc,\nNearest to on: in, for, vocals, and, from, at, with, to,\nNearest to or: and, of, reginae, UNK, sigma, the, agave, available,\nNearest to new: hurd, son, fuel, stephen, reaching, homosexual, everything, be,\nNearest to state: gone, lymphoma, spread, defeated, comprised, subject, foods, symptoms,\nNearest to an: accessible, agave, life, fate, angle, observations, subsequently, the,\nNearest to three: one, vocals, two, six, eight, agave, archie, nine,\nNearest to of: and, in, for, from, agave, rotate, accessible, throne,\nAverage loss at step  12000 :  13.9242232451\nAverage loss at step  14000 :  11.7597276529\nAverage loss at step  16000 :  9.75443382001\nAverage loss at step  18000 :  8.44891561031\nAverage loss at step  20000 :  8.0076307404\nNearest to can: aleksandar, art, ep, might, would, may, garth, but,\nNearest to while: and, in, for, promotes, brake, unarmed, sends, bart,\nNearest to however: complementary, girardeau, arguments, imran, bronchitis, updated, vocals, yields,\nNearest to some: the, any, satisfy, anatomy, mothers, mango, bckgr, transvaal,\nNearest to was: is, were, has, by, had, are, dasyprocta, nine,\nNearest to united: circ, abortion, quantify, geology, wallpaper, field, carries, agouti,\nNearest to for: and, of, in, to, on, as, vocals, from,\nNearest to will: adhemar, oldest, here, sprint, seaborg, town, were, coke,\nNearest to eight: nine, six, zero, seven, dasyprocta, five, four, three,\nNearest to on: in, and, from, two, for, at, with, vocals,\nNearest to or: and, UNK, dasyprocta, zero, of, apatosaurus, agouti, reginae,\nNearest to new: hurd, son, homosexual, ambiguous, fuel, formula, everything, smell,\nNearest to state: amazonas, gone, lymphoma, comprised, defeated, spread, dundee, dasyprocta,\nNearest to an: accessible, the, aglaulus, agave, dissertation, fate, advisor, merck,\nNearest to three: two, eight, six, nine, four, zero, seven, dasyprocta,\nNearest to of: in, and, for, from, apatosaurus, dasyprocta, zero, nine,\nAverage loss at step  22000 :  7.00365502906\nAverage loss at step  24000 :  6.84823495102\nAverage loss at step  26000 :  6.7255476979\nAverage loss at step  28000 :  6.35838697827\nAverage loss at step  30000 :  5.94163178968\nNearest to can: may, would, might, aleksandar, art, cannot, to, schoenberg,\nNearest to while: and, promotes, emulators, sends, unarmed, for, airbus, dasyprocta,\nNearest to however: complementary, arguments, imran, girardeau, bronchitis, vocals, abet, mishnayot,\nNearest to some: the, any, many, satisfy, immunoglobulin, mothers, resources, heuristics,\nNearest to was: is, were, has, had, by, are, dasyprocta, although,\nNearest to united: quantify, abortion, circ, morphism, geology, wallpaper, field, handwriting,\nNearest to for: and, in, of, to, from, on, almighty, by,\nNearest to will: adhemar, were, here, seaborg, oldest, should, sprint, coke,\nNearest to eight: nine, six, seven, five, four, zero, dasyprocta, three,\nNearest to on: in, from, at, for, two, with, vocals, and,\nNearest to or: and, dasyprocta, fao, agouti, apatosaurus, reginae, the, UNK,\nNearest to new: hurd, son, arin, homosexual, blanc, ambiguous, everything, apatosaurus,\nNearest to state: amazonas, comprised, gone, lymphoma, foods, dundee, defeated, intermixed,\nNearest to an: accessible, the, dissertation, aglaulus, agave, oysters, truetype, advisor,\nNearest to three: six, four, two, eight, five, seven, nine, zero,\nNearest to of: in, and, apatosaurus, mishnayot, for, dasyprocta, from, rotate,\nAverage loss at step  32000 :  6.00780239582\nAverage loss at step  34000 :  5.67529272211\nAverage loss at step  36000 :  5.76577446926\nAverage loss at step  38000 :  5.52025156832\nAverage loss at step  40000 :  5.27679153919\nNearest to can: may, would, might, will, cannot, to, aleksandar, could,\nNearest to while: and, or, promotes, unarmed, sends, emulators, airbus, in,\nNearest to however: arguments, complementary, imran, bronchitis, that, girardeau, abet, but,\nNearest to some: many, the, any, satisfy, their, abakan, this, these,\nNearest to was: is, has, were, had, by, are, dasyprocta, although,\nNearest to united: quantify, circ, abortion, morphism, geology, wallpaper, handwriting, gulag,\nNearest to for: of, and, in, almighty, vocals, dasyprocta, or, metroid,\nNearest to will: can, may, should, would, adhemar, could, here, seaborg,\nNearest to eight: six, nine, seven, four, zero, five, three, dasyprocta,\nNearest to on: in, from, at, and, two, upon, vocals, between,\nNearest to or: and, dasyprocta, fao, zero, agouti, basins, abet, recitative,\nNearest to new: nour, arin, son, hurd, bouvet, homosexual, blanc, everything,\nNearest to state: amazonas, lymphoma, gone, intermixed, comprised, foods, agouti, dasyprocta,\nNearest to an: accessible, the, aglaulus, truetype, oysters, dissertation, peacekeeping, desolate,\nNearest to three: four, six, five, two, seven, eight, zero, one,\nNearest to of: apatosaurus, zero, in, dasyprocta, mishnayot, and, for, abet,\nAverage loss at step  42000 :  5.36507705772\nAverage loss at step  44000 :  5.26350083554\nAverage loss at step  46000 :  5.23372088957\nAverage loss at step  48000 :  5.21567833161\nAverage loss at step  50000 :  5.01955409896\nNearest to can: may, would, will, might, cannot, could, should, to,\nNearest to while: and, or, for, dasyprocta, sends, during, emulators, promotes,\nNearest to however: arguments, but, bronchitis, and, imran, mishnayot, abet, that,\nNearest to some: many, any, these, the, two, their, this, several,\nNearest to was: is, has, were, had, by, dasyprocta, are, midas,\nNearest to united: quantify, circ, morphism, wallpaper, abortion, handwriting, geology, gulag,\nNearest to for: of, in, dasyprocta, with, and, vocals, almighty, or,\nNearest to will: can, may, would, should, could, two, epiphanes, here,\nNearest to eight: six, seven, nine, five, four, three, dasyprocta, zero,\nNearest to on: in, from, at, two, circ, agouti, upon, operatorname,\nNearest to or: and, dasyprocta, kapoor, three, fao, six, four, eight,\nNearest to new: nour, arin, son, bouvet, blanc, homosexual, nightmare, hurd,\nNearest to state: amazonas, intermixed, comprised, gone, lymphoma, foods, christian, surrounds,\nNearest to an: accessible, dissertation, aglaulus, oysters, truetype, agave, peacekeeping, the,\nNearest to three: four, six, seven, five, eight, two, kapoor, dasyprocta,\nNearest to of: in, apatosaurus, for, abet, dasyprocta, mishnayot, kapoor, nine,\nAverage loss at step  52000 :  5.0476544975\nAverage loss at step  54000 :  5.19588781452\nAverage loss at step  56000 :  5.04974017704\nAverage loss at step  58000 :  5.04379871094\nAverage loss at step  60000 :  4.92641462922\nNearest to can: may, would, will, might, cannot, could, should, to,\nNearest to while: and, but, or, although, dasyprocta, during, sends, however,\nNearest to however: but, that, arguments, abet, mishnayot, bronchitis, pulau, imran,\nNearest to some: many, these, any, several, their, two, three, the,\nNearest to was: is, were, has, had, by, been, dasyprocta, electrified,\nNearest to united: quantify, circ, morphism, handwriting, british, wallpaper, geology, abortion,\nNearest to for: of, almighty, or, dasyprocta, in, with, including, vocals,\nNearest to will: can, would, may, should, could, cannot, epiphanes, to,\nNearest to eight: nine, six, seven, four, five, zero, dasyprocta, three,\nNearest to on: in, at, from, upon, two, circ, pandemic, ursus,\nNearest to or: and, dasyprocta, pulau, kapoor, fao, basins, ursus, six,\nNearest to new: nour, son, arin, nightmare, bouvet, blanc, bckgr, homosexual,\nNearest to state: amazonas, intermixed, comprised, lymphoma, gone, surrounds, foods, agouti,\nNearest to an: accessible, dissertation, desolate, defective, truetype, agave, india, dracula,\nNearest to three: four, six, five, two, seven, eight, kapoor, dasyprocta,\nNearest to of: apatosaurus, in, dasyprocta, mishnayot, and, six, abet, nine,\nAverage loss at step  62000 :  5.00237227571\nAverage loss at step  64000 :  4.82625176424\nAverage loss at step  66000 :  4.58996698523\nAverage loss at step  68000 :  4.98107554257\nAverage loss at step  70000 :  4.88743628001\nNearest to can: may, would, will, might, could, cannot, should, to,\nNearest to while: and, but, although, dasyprocta, during, or, however, when,\nNearest to however: but, arguments, that, when, mishnayot, abet, while, bronchitis,\nNearest to some: many, these, any, several, their, the, pulau, all,\nNearest to was: is, has, had, were, when, by, became, dasyprocta,\nNearest to united: quantify, circ, morphism, handwriting, wallpaper, british, gulag, abortion,\nNearest to for: of, including, almighty, in, vocals, dasyprocta, ursus, ulyanov,\nNearest to will: can, would, may, should, could, cannot, epiphanes, might,\nNearest to eight: six, nine, seven, four, five, three, zero, dasyprocta,\nNearest to on: in, upon, from, circ, at, agouti, ursus, operatorname,\nNearest to or: and, dasyprocta, fao, basins, ursus, kapoor, pulau, but,\nNearest to new: nour, bouvet, arin, nightmare, bckgr, blanc, thaler, son,\nNearest to state: amazonas, intermixed, lymphoma, bokassa, surrounds, comprised, agouti, kuwaiti,\nNearest to an: accessible, thaler, desolate, the, microcebus, defective, aglaulus, observations,\nNearest to three: four, six, five, two, seven, eight, thaler, kapoor,\nNearest to of: apatosaurus, mishnayot, dasyprocta, and, ursus, abet, for, agouti,\nAverage loss at step  72000 :  4.76344278848\nAverage loss at step  74000 :  4.81809603071\nAverage loss at step  76000 :  4.73472615635\nAverage loss at step  78000 :  4.79428102446\nAverage loss at step  80000 :  4.79969809246\nNearest to can: may, would, will, might, could, cannot, should, must,\nNearest to while: and, but, although, however, during, dasyprocta, when, or,\nNearest to however: but, that, when, arguments, mishnayot, iit, abet, while,\nNearest to some: many, these, any, several, the, their, all, this,\nNearest to was: is, has, were, had, became, when, by, although,\nNearest to united: quantify, morphism, circ, handwriting, wallpaper, british, gulag, geology,\nNearest to for: almighty, of, imran, ulyanov, dasyprocta, when, or, ursus,\nNearest to will: can, would, may, should, could, cannot, must, might,\nNearest to eight: nine, seven, six, five, four, three, zero, dasyprocta,\nNearest to on: in, upon, circ, at, from, agouti, ursus, two,\nNearest to or: and, dasyprocta, fao, basins, ursus, but, kapoor, pulau,\nNearest to new: nour, bouvet, arin, nightmare, thaler, crb, bckgr, adjoint,\nNearest to state: amazonas, lymphoma, bokassa, iit, surrounds, agouti, interfere, intermixed,\nNearest to an: accessible, thaler, the, dracula, oysters, microcebus, desolate, peacekeeping,\nNearest to three: four, six, five, two, seven, eight, thaler, kapoor,\nNearest to of: apatosaurus, abet, in, dasyprocta, ursus, iit, kapoor, pulau,\nAverage loss at step  82000 :  4.76486732888\nAverage loss at step  84000 :  4.76449827051\nAverage loss at step  86000 :  4.76764528954\nAverage loss at step  88000 :  4.75402819717\nAverage loss at step  90000 :  4.73079549515\nNearest to can: may, would, will, might, could, cannot, should, must,\nNearest to while: and, although, but, however, dasyprocta, or, when, during,\nNearest to however: but, that, while, mishnayot, when, iit, abet, arguments,\nNearest to some: many, these, several, any, the, various, their, all,\nNearest to was: is, had, has, were, by, became, when, although,\nNearest to united: quantify, circ, morphism, handwriting, wallpaper, british, gulag, indian,\nNearest to for: dasyprocta, globemaster, almighty, during, imran, when, including, ulyanov,\nNearest to will: can, would, may, should, could, cannot, might, must,\nNearest to eight: seven, five, nine, six, four, three, zero, dasyprocta,\nNearest to on: in, upon, from, at, circ, through, ursus, agouti,\nNearest to or: and, dasyprocta, fao, kapoor, ursus, iit, but, four,\nNearest to new: nour, bouvet, nightmare, elwes, thaler, crb, arin, blanc,\nNearest to state: amazonas, bokassa, iit, lymphoma, agouti, surrounds, interfere, intermixed,\nNearest to an: accessible, thaler, peacekeeping, dracula, oysters, vms, dissertation, desolate,\nNearest to three: four, five, six, seven, two, eight, one, kapoor,\nNearest to of: apatosaurus, in, dasyprocta, ursus, mishnayot, iit, abet, including,\nAverage loss at step  92000 :  4.65430845106\nAverage loss at step  94000 :  4.7287168777\nAverage loss at step  96000 :  4.68045081711\nAverage loss at step  98000 :  4.58237247276\nAverage loss at step  100000 :  4.70639153123\nNearest to can: may, will, would, could, might, cannot, should, must,\nNearest to while: although, but, and, however, when, or, during, dasyprocta,\nNearest to however: but, while, that, mishnayot, when, iit, arguments, though,\nNearest to some: many, these, several, the, any, their, various, this,\nNearest to was: is, had, has, were, became, be, dasyprocta, by,\nNearest to united: constituci, quantify, morphism, circ, handwriting, wallpaper, indian, maneuvering,\nNearest to for: of, dasyprocta, almighty, in, globemaster, when, ursus, including,\nNearest to will: can, would, may, could, should, cannot, might, must,\nNearest to eight: seven, six, nine, five, four, three, zero, dasyprocta,\nNearest to on: in, upon, from, circ, through, at, two, pandemic,\nNearest to or: and, dasyprocta, kapoor, pulau, fao, but, ursus, iit,\nNearest to new: nour, bouvet, nightmare, chiropractors, crb, elwes, thaler, blanc,\nNearest to state: amazonas, constituci, bokassa, iit, agouti, surrounds, lymphoma, interfere,\nNearest to an: accessible, thaler, dracula, peacekeeping, oysters, amuse, angle, vms,\nNearest to three: five, four, six, seven, two, eight, kapoor, dasyprocta,\nNearest to of: apatosaurus, dasyprocta, abet, iit, ursus, including, in, mishnayot,\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 10, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Step 6: Visualize the embeddings.\n\n\ndef plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n  plt.figure(figsize=(18, 18))  # in inches\n  for i, label in enumerate(labels):\n    x, y = low_dim_embs[i, :]\n    plt.scatter(x, y)\n    plt.annotate(label,\n                 xy=(x, y),\n                 xytext=(5, 2),\n                 textcoords='offset points',\n                 ha='right',\n                 va='bottom')\n\n  plt.savefig(filename)\n\ntry:\n  # pylint: disable=g-import-not-at-top\n  from sklearn.manifold import TSNE\n  import matplotlib.pyplot as plt\n\n  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n  plot_only = 500\n  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n  plot_with_labels(low_dim_embs, labels)\n\nexcept ImportError:\n  print('Please install sklearn, matplotlib, and scipy to show embeddings.')", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "outputs": [], 
            "cell_type": "code"
        }
    ]
}